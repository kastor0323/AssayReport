# resume_evaluator.py
import pandas as pd
from kiwipiepy import Kiwi
from collections import Counter
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tag import pos_tag
from flask import Flask, request, jsonify
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

class ResumeEvaluator:
    def __init__(self, reference_csv_path='jobkorea_keyword_analysis.csv'):
        """
        ìì†Œì„œ í‰ê°€ê¸° ì´ˆê¸°í™”
        Args:
            reference_csv_path: í•©ê²© ìì†Œì„œ ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ
        """
        self.reference_df = pd.read_csv(reference_csv_path, encoding='utf-8-sig')
        self.kiwi = Kiwi()
        
        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œì—ë§Œ í•„ìš”)
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
            nltk.data.find('taggers/averaged_perceptron_tagger')
        except LookupError:
            nltk.download('punkt')
            nltk.download('stopwords')
            nltk.download('averaged_perceptron_tagger')
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if pd.isna(text) or text == '':
            return ''
        
        text_str = str(text)
        text_str = text_str.replace('\x00', '')
        text_str = text_str.replace('\0', '')
        
        return text_str.strip()
    
    def extract_words(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ (Kiwi ì‚¬ìš©)"""
        if pd.isna(text) or text == '':
            return []
        
        text = self.clean_text(text)
        
        # ì˜ì–´ ë¹„ìœ¨ í™•ì¸ (extract_keywords_multilingualê³¼ ìœ ì‚¬í•œ ë¡œì§)
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(re.findall(r'[a-zA-Zê°€-í£]', text))
        is_english = english_chars / max(total_chars, 1) > 0.5

        if is_english:
            # ì˜ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            cleaned = re.sub(r'[^\w\s]', ' ', text)
            cleaned = re.sub(r'\s+', ' ', cleaned).strip().lower()
            words = [word for word in cleaned.split() if len(word) >= 2 and word.isalpha()]
            return words
        else:
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Kiwi ì‚¬ìš©)
            try:
                tokens = self.kiwi.tokenize(text)
                # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“± ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ ì¶”ì¶œ
                words = [token.form for token in tokens
                         if token.tag.startswith(('N', 'V', 'J')) and len(token.form) >= 2]
                return words
            except Exception as e:
                print(f"Kiwi extract_words ì˜¤ë¥˜: {e}")
                return []
    
    def word_based_similarity(self, q1, q2):
        """ë‘ ì§ˆë¬¸ ê°„ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        # q1ê³¼ q2ê°€ ë¬¸ìì—´ì´ê³  NaNì´ ì•„ë‹Œì§€ í™•ì¸
        q1_cleaned = self.clean_text(q1)
        q2_cleaned = self.clean_text(q2)

        # í•µì‹¬ í‚¤ì›Œë“œì— ëŒ€í•œ ì§ì ‘ì ì¸ ë¶€ë¶„ ë¬¸ìì—´ ì¼ì¹˜ í™•ì¸
        # ì˜ˆ: "ìê¸°ì†Œê°œë¥¼ í•´ë³´ì„¸ìš”"ì™€ "ìê¸°ì†Œê°œ"ì˜ ê²½ìš°
        if q2_cleaned and q1_cleaned and q2_cleaned in q1_cleaned:
            return 1.0 # ì§ì ‘ì ì¸ í‚¤ì›Œë“œ ì¼ì¹˜ì— ëŒ€í•´ ë†’ì€ ìœ ì‚¬ë„ ë¶€ì—¬

        words1 = set(self.extract_words(q1_cleaned))
        words2 = set(self.extract_words(q2_cleaned))

        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def extract_keywords_multilingual(self, answers, top_n=20):
        """í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        korean_nouns = []
        english_words = []
        
        try:
            english_stopwords = set(stopwords.words('english'))
        except:
            english_stopwords = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])
        
        for answer in answers:
            if pd.isna(answer) or answer == '':
                continue
            
            answer_text = self.clean_text(answer)
            if not answer_text:
                continue
            
            cleaned_answer = re.sub(r'[^\w\sê°€-í£]', ' ', answer_text)
            cleaned_answer = re.sub(r'\s+', ' ', cleaned_answer).strip()
            
            # ì˜ì–´ ë¹„ìœ¨ í™•ì¸
            english_chars = len(re.findall(r'[a-zA-Z]', cleaned_answer))
            total_chars = len(re.findall(r'[a-zA-Zê°€-í£]', cleaned_answer))
            is_english = english_chars / max(total_chars, 1) > 0.5
            
            if is_english:
                # ì˜ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                try:
                    tokens = word_tokenize(cleaned_answer.lower())
                    pos_tags = pos_tag(tokens)
                    words = [word for word, pos in pos_tags 
                            if pos.startswith(('NN', 'JJ', 'VB')) 
                            and len(word) >= 2 
                            and word not in english_stopwords
                            and word.isalpha()]
                    english_words.extend(words)
                except:
                    words = [word.lower() for word in cleaned_answer.split() 
                            if len(word) >= 2 and word.lower() not in english_stopwords and word.isalpha()]
                    english_words.extend(words)
            else:
                # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                try:
                    tokens = self.kiwi.tokenize(cleaned_answer)
                    nouns = [token.form for token in tokens 
                            if token.tag.startswith('N') and len(token.form) >= 2]
                    korean_nouns.extend(nouns)
                except Exception as e:
                    print(f"Kiwi ë¶„ì„ ì˜¤ë¥˜: {e}")
                    continue
        
        all_words = korean_nouns + english_words
        
        if not all_words:
            return []
        
        counter = Counter(all_words)
        extracted_keywords = [word for word, count in counter.most_common(top_n)]
        return extracted_keywords
    
    def find_best_matching_question(self, user_question, job_title, position):
        """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ í•©ê²© ìì†Œì„œ ì§ˆë¬¸ ì°¾ê¸°"""
        # ì •ê·œí™” ì œê±°: ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # í•´ë‹¹ ì§ë¬´/ì§ìœ„ ë°ì´í„° í•„í„°ë§ (ì •í™•í•œ ë§¤ì¹­)
        filtered_df = self.reference_df[
            (self.reference_df['ì§ë¬´'] == job_title) & 
            (self.reference_df['ì§ìœ„'] == position)
        ]
        
        if filtered_df.empty:
            print(f"âŒ '{job_title} - {position}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            available_jobs = self.reference_df.groupby(['ì§ë¬´', 'ì§ìœ„']).size().reset_index(name='count')
            print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì§ë¬´/ì§ìœ„:")
            for _, row in available_jobs.iterrows():
                keywords_for_job = self.reference_df[
                    (self.reference_df['ì§ë¬´'] == row['ì§ë¬´']) &
                    (self.reference_df['ì§ìœ„'] == row['ì§ìœ„'])
                ]['í•µì‹¬ë‹¨ì–´'].tolist()
                print(f"  - {row['ì§ë¬´']} ({row['ì§ìœ„']}) - í•µì‹¬ë‹¨ì–´: {', '.join(keywords_for_job)}")
            return None
        
        max_similarity = 0
        best_match_keyword = None
        best_match_data = None
        
        # ì§€ì›ë™ê¸°, íšŒì‚¬ì„ íƒ ë“± í•µì‹¬ í‚¤ì›Œë“œ ìš°ì„  ë§¤ì¹­
        priority_keywords = ['ì§€ì›ë™ê¸°', 'íšŒì‚¬ì„ íƒ', 'ì…ì‚¬ë™ê¸°']
        for keyword in priority_keywords:
            if keyword in user_question.lower():
                matching_rows = filtered_df[filtered_df['í•µì‹¬ë‹¨ì–´'] == keyword]
                if not matching_rows.empty:
                    best_match_keyword = keyword
                    best_match_data = matching_rows.iloc[0]
                    return best_match_keyword, 1.0, best_match_data
        
        # ì¼ë°˜ì ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
        for idx, row in filtered_df.iterrows():
            similarity = self.word_based_similarity(user_question, row['í•µì‹¬ë‹¨ì–´'])
            if similarity > max_similarity:
                max_similarity = similarity
                best_match_keyword = row['í•µì‹¬ë‹¨ì–´']
                best_match_data = row
        
        return best_match_keyword, max_similarity, best_match_data
    
    def calculate_keyword_matching_score(self, user_answer, reference_keywords):
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        user_keywords = self.extract_keywords_multilingual([user_answer], top_n=20)
        
        if not reference_keywords or pd.isna(reference_keywords):
            return 0.0, []
        
        reference_keyword_list = [kw.strip() for kw in str(reference_keywords).split(',')]
        reference_keyword_set = set(reference_keyword_list)
        user_keyword_set = set(user_keywords)
        
        if not reference_keyword_set or not user_keyword_set:
            return 0.0, []
        
        # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ ê³„ì‚°
        matched_keywords = user_keyword_set.intersection(reference_keyword_set)
        matched_count = len(matched_keywords)
        
        # ì ìˆ˜ ê³„ì‚° ë°©ì‹ ë³€ê²½
        # 20ê°œ ì¤‘ ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¼ ì ìˆ˜ ë¶€ì—¬
        # 16ê°œ ì´ìƒ: 100ì 
        # 12-15ê°œ: 80-95ì 
        # 8-11ê°œ: 60-75ì 
        # 4-7ê°œ: 40-55ì 
        # 1-3ê°œ: 20-35ì 
        # 0ê°œ: 0ì 
        
        if matched_count >= 16:
            score = 100
        elif matched_count >= 12:
            score = 80 + (matched_count - 12) * 5
        elif matched_count >= 8:
            score = 60 + (matched_count - 8) * 5
        elif matched_count >= 4:
            score = 40 + (matched_count - 4) * 5
        elif matched_count >= 1:
            score = 20 + (matched_count - 1) * 5
        else:
            score = 0
        
        return score, list(matched_keywords)
    
    def evaluate_resume(self, user_data):
        """
        ì‚¬ìš©ì ìì†Œì„œ í‰ê°€
        Args:
            user_data: {
                'ì§ë¬´': 'ì§ë¬´ëª…',
                'ì§ìœ„': 'ì§ìœ„ëª…', 
                'qa_pairs': [
                    {'question': 'ì§ˆë¬¸1', 'answer': 'ë‹µë³€1'},
                    {'question': 'ì§ˆë¬¸2', 'answer': 'ë‹µë³€2'},
                    ...
                ]
            }
        Returns:
            dict: {
                'í‰ê· ì ìˆ˜': float,
                'ë“±ê¸‰': str,
                'ìƒì„¸ê²°ê³¼': list
            }
        """
        job_title = user_data['ì§ë¬´']
        position = user_data['ì§ìœ„']
        qa_pairs = user_data['qa_pairs']
        
        # í•´ë‹¹ ì§ë¬´/ì§ìœ„ ë°ì´í„° í™•ì¸
        available_data = self.reference_df[
            (self.reference_df['ì§ë¬´'] == job_title) & 
            (self.reference_df['ì§ìœ„'] == position)
        ]
        
        if available_data.empty:
            print(f"âŒ '{job_title} - {position}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            available_jobs = self.reference_df.groupby(['ì§ë¬´', 'ì§ìœ„']).size().reset_index(name='count')
            print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì§ë¬´/ì§ìœ„:")
            for _, row in available_jobs.iterrows():
                keywords_for_job = self.reference_df[
                    (self.reference_df['ì§ë¬´'] == row['ì§ë¬´']) &
                    (self.reference_df['ì§ìœ„'] == row['ì§ìœ„'])
                ]['í•µì‹¬ë‹¨ì–´'].tolist()
                print(f"  - {row['ì§ë¬´']} ({row['ì§ìœ„']}) - í•µì‹¬ë‹¨ì–´: {', '.join(keywords_for_job)}")
            return None
        
        evaluation_results = []
        
        for i, qa in enumerate(qa_pairs):
            user_question = qa['question']
            user_answer = qa['answer']
            
            # ê°€ì¥ ìœ ì‚¬í•œ í•µì‹¬ë‹¨ì–´ ì°¾ê¸°
            best_keyword, _, best_data = self.find_best_matching_question(
                user_question, job_title, position
            )
            
            if best_data is not None:
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_score, matched_keywords = self.calculate_keyword_matching_score(
                    user_answer, best_data['ë‹µë³€í‚¤ì›Œë“œ_TOP20']
                )
                
                evaluation_results.append({
                    'ì§ˆë¬¸ë²ˆí˜¸': i + 1,
                    'ì‚¬ìš©ìì§ˆë¬¸': user_question,
                    'ì‚¬ìš©ìë‹µë³€': user_answer[:50] + "..." if len(user_answer) > 50 else user_answer,
                    'ê°€ì¥ìœ ì‚¬í•œì§ˆë¬¸': best_keyword,
                    'ë§¤ì¹­ëœí‚¤ì›Œë“œ': ', '.join(matched_keywords),
                    'ë§¤ì¹­ëœí‚¤ì›Œë“œìˆ˜': len(matched_keywords),
                    'ì¢…í•©ì ìˆ˜': round(keyword_score, 1)
                })
            else:
                evaluation_results.append({
                    'ì§ˆë¬¸ë²ˆí˜¸': i + 1,
                    'ì‚¬ìš©ìì§ˆë¬¸': user_question,
                    'ì‚¬ìš©ìë‹µë³€': user_answer[:50] + "..." if len(user_answer) > 50 else user_answer,
                    'ê°€ì¥ìœ ì‚¬í•œì§ˆë¬¸': 'N/A',
                    'ë§¤ì¹­ëœí‚¤ì›Œë“œ': '',
                    'ë§¤ì¹­ëœí‚¤ì›Œë“œìˆ˜': 0,
                    'ì¢…í•©ì ìˆ˜': 0.0
                })
        
        # ê²°ê³¼ ì •ë¦¬
        result_df = pd.DataFrame(evaluation_results)
        average_score = result_df['ì¢…í•©ì ìˆ˜'].mean()
        
        # ì ìˆ˜ë³„ ë“±ê¸‰ ê²°ì •
        if average_score >= 80:
            grade = "ìš°ìˆ˜ (80ì  ì´ìƒ)"
        elif average_score >= 60:
            grade = "ë³´í†µ (60-79ì )"
        elif average_score >= 40:
            grade = "ë¯¸í¡ (40-59ì )"
        else:
            grade = "ë¶€ì¡± (40ì  ë¯¸ë§Œ)"
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        return {
            'í‰ê· ì ìˆ˜': round(average_score, 1),
            'ë“±ê¸‰': grade,
            'ìƒì„¸ê²°ê³¼': evaluation_results
        }

@app.route('/evaluate', methods=['POST'])
def evaluate_resume():
    try:
        data = request.get_json()
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ['ì§ë¬´', 'ì§ìœ„', 'qa_pairs']
        for field in required_fields:
            if field not in data:
                return jsonify({'error': f'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {field}'}), 400

        # í‰ê°€ê¸° ì´ˆê¸°í™”
        evaluator = ResumeEvaluator('jobkorea_keyword_analysis.csv')
        
        # ìì†Œì„œ í‰ê°€ ì‹¤í–‰
        result = evaluator.evaluate_resume(data)
        
        if result is None:
            return jsonify({'error': 'í‰ê°€ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 400
            
        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000, debug=True)
