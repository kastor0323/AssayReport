#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¡ì½”ë¦¬ì•„ í•©ê²©ìì†Œì„œ ìŠ¤í¬ë˜í•‘ í”„ë¡œê·¸ë¨
NLP_Project.ipynb ë°©ì‹ ì™„ì „ í†µí•© ë²„ì „
Python 3.13.3 í˜¸í™˜
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import datetime
from urllib.parse import urljoin, urlparse
import logging
import getpass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JobkoreaScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def login_to_jobkorea(self, user_id, password):
        """ì¡ì½”ë¦¬ì•„ ë¡œê·¸ì¸"""
        try:
            logger.info("ì¡ì½”ë¦¬ì•„ ë¡œê·¸ì¸ ì‹œë„...")
            
            # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì— ì ‘ì†í•´ì„œ ì¿ í‚¤ë¥¼ ë°›ì•„ì˜´
            main_url = "https://www.jobkorea.co.kr/"
            response = self.session.get(main_url)
            
            if response.status_code != 200:
                logger.error(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                return False
            
            # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™
            login_url = "https://www.jobkorea.co.kr/Login/Login_Tot.asp?rDBName=GG&re_url=%2f%3fschTxt%3d%26Page%3d"
            response = self.session.get(login_url)
            
            if response.status_code != 200:
                logger.error(f"ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                return False
            
            soup = BeautifulSoup(response.content, 'html.parser')
            logger.info(f"ë¡œê·¸ì¸ í˜ì´ì§€ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            
            # ë¡œê·¸ì¸ í¼ì—ì„œ í•„ìš”í•œ ë°ì´í„° ì°¾ê¸°
            login_form = soup.find('form')
            if not login_form:
                logger.error("ë¡œê·¸ì¸ í¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # í¼ ì•¡ì…˜ URL ì°¾ê¸°
            action_url = login_form.get('action', '/Login/Login.asp')
            if not action_url.startswith('http'):
                action_url = "https://www.jobkorea.co.kr" + action_url
            
            # ë¡œê·¸ì¸ í¼ ë°ì´í„° ì¤€ë¹„ - ì¡ì½”ë¦¬ì•„ ì‹¤ì œ í•„ë“œëª… ì‚¬ìš©
            login_data = {
                'M_ID': user_id,
                'M_PWD': password,
                'rDBName': 'GG',
                'LoginType': 'Members'
            }
            
            # í¼ì—ì„œ ì¶”ê°€ hidden í•„ë“œë“¤ ì°¾ê¸°
            hidden_inputs = soup.find_all('input', type='hidden')
            for hidden_input in hidden_inputs:
                name = hidden_input.get('name')
                value = hidden_input.get('value', '')
                if name and name not in login_data:
                    login_data[name] = value
            
            # ë¡œê·¸ì¸ ìš”ì²­
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Content-Type': 'application/x-www-form-urlencoded',
                'Referer': login_url
            }
            
            response = self.session.post(action_url, data=login_data, headers=headers, allow_redirects=True)
            logger.info(f"ë¡œê·¸ì¸ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            logger.info(f"ìµœì¢… URL: {response.url}")
            
            # ì‹¤ì œ ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ í•©ê²©ìì†Œì„œ í˜ì´ì§€ ì ‘ê·¼ ì‹œë„
            passassay_url = "https://www.jobkorea.co.kr/starter/passassay"
            test_response = self.session.get(passassay_url)
            
            logger.info(f"í•©ê²©ìì†Œì„œ í˜ì´ì§€ ì ‘ê·¼ ì½”ë“œ: {test_response.status_code}")
            
            if test_response.status_code == 200:
                soup = BeautifulSoup(test_response.content, 'html.parser')
                
                # ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë‹¤ëŠ” ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                login_required_indicators = [
                    'ë¡œê·¸ì¸ì´ í•„ìš”',
                    'ë¡œê·¸ì¸ í›„ ì´ìš©',
                    'íšŒì›ê°€ì…',
                    'login',
                    'ë¡œê·¸ì¸í•˜ì„¸ìš”'
                ]
                
                page_text = soup.get_text()
                needs_login = any(indicator in page_text for indicator in login_required_indicators)
                
                # í•©ê²©ìì†Œì„œ ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                passassay_indicators = [
                    'í•©ê²©ìì†Œì„œ',
                    'ìê¸°ì†Œê°œì„œ',
                    'ë©´ì ‘í›„ê¸°',
                    'íšŒì‚¬ëª…',
                    'ì§€ì›ì§ë¬´'
                ]
                
                has_passassay_content = any(indicator in page_text for indicator in passassay_indicators)
                
                if has_passassay_content and not needs_login:
                    logger.info("âœ… ë¡œê·¸ì¸ ì„±ê³µ! (í•©ê²©ìì†Œì„œ í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥)")
                    return True
                elif needs_login:
                    logger.error("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: ì—¬ì „íˆ ë¡œê·¸ì¸ì´ í•„ìš”í•¨")
                    return False
                else:
                    logger.warning("âš ï¸ ë¡œê·¸ì¸ ìƒíƒœ ë¶ˆë¶„ëª…, ê³„ì† ì§„í–‰...")
                    return True  # ì¼ë‹¨ ì„±ê³µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì§„í–‰
            else:
                logger.error(f"âŒ í•©ê²©ìì†Œì„œ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {test_response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
            return False

    def extract_company_name_from_link(self, soup):
        """ê¸°ì—… í™ˆ ì´ë™ ë§í¬ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ - NLP_Project ë°©ì‹"""
        company_name = ""
        
        try:
            # NLP_Projectì—ì„œ ì‚¬ìš©í•œ ì…€ë ‰í„°ë“¤ì„ BeautifulSoupìš©ìœ¼ë¡œ ë³€í™˜
            company_link_selectors = [
                'a[title="ê¸°ì—… í™ˆ ì´ë™"]',
                'a[href*="/Recruit/Co_Read/C/"]',
                'a[target="_blank"][title="ê¸°ì—… í™ˆ ì´ë™"]'
            ]
            
            for selector in company_link_selectors:
                try:
                    company_link = soup.select_one(selector)
                    if company_link:
                        company_name = company_link.get_text(strip=True)
                        if company_name and len(company_name) > 1:
                            logger.info(f"ê¸°ì—… í™ˆ ë§í¬ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ: {company_name}")
                            return company_name
                except Exception as e:
                    logger.debug(f"ì…€ë ‰í„° {selector} ì˜¤ë¥˜: {e}")
                    continue
                    
            logger.warning("âŒ íšŒì‚¬ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""
            
        except Exception as e:
            logger.error(f"ê¸°ì—… í™ˆ ë§í¬ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def extract_position_text_from_em(self, soup):
        """em íƒœê·¸ì—ì„œ ì§ë¬´ ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ - NLP_Project ë°©ì‹"""
        position_text = ""
        
        try:
            # 1. em íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„)
            em_elements = soup.find_all("em")
            for elem in em_elements:
                try:
                    text = elem.get_text(strip=True)
                    if text and len(text) > 3:
                        position_text = text
                        logger.info(f"em íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ: {text}")
                        return position_text
                except:
                    continue
            
            # 2. em íƒœê·¸ì—ì„œ ëª» ì°¾ìœ¼ë©´ titleì—ì„œ ì¶”ì¶œ
            if not position_text:
                try:
                    # BeautifulSoupì—ì„œëŠ” soup.title.string ì‚¬ìš©
                    title_tag = soup.find('title')
                    if title_tag:
                        position_text = title_tag.get_text(strip=True)
                        logger.info(f"ì œëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ: {position_text}")
                    else:
                        position_text = ""
                except:
                    position_text = ""
                    
            # 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ h1, h2 íƒœê·¸ì—ì„œ ì°¾ê¸°
            if not position_text:
                for tag_name in ['h1', 'h2']:
                    try:
                        header_elem = soup.find(tag_name)
                        if header_elem:
                            text = header_elem.get_text(strip=True)
                            if text and len(text) > 3:
                                position_text = text
                                logger.info(f"{tag_name} íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ: {text}")
                                break
                    except:
                        continue
                        
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            position_text = ""
        
        return position_text

    def extract_position_info(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì—°ë„/ê¸°ê°„, ìƒíƒœ, ì§ì—… ë¶„ë¦¬ - NLP_Project ë°©ì‹ ê·¸ëŒ€ë¡œ"""
        year_period = ""
        status = ""
        job = ""
        
        try:
            # ì—°ë„/ê¸°ê°„ ì¶”ì¶œ
            year_period_match = re.search(r"\d{4}ë…„?\s*[ìƒí•˜]ë°˜ê¸°", text)
            if year_period_match:
                year_period = year_period_match.group(0)
            
            # ìƒíƒœ ì¶”ì¶œ (ì¸í„´, ì‹ ì…ë§Œ)
            status_keywords = ["ì¸í„´", "ì‹ ì…"]
            for keyword in status_keywords:
                if keyword in text:
                    status = keyword
                    break
            
            # ì§ì—… ì¶”ì¶œ - NLP_Project ë°©ì‹ ê·¸ëŒ€ë¡œ
            words = text.split()
            for word in words:
                # ìƒíƒœ í‚¤ì›Œë“œ ì œì™¸
                if word in status_keywords:
                    continue
                # ì—°ë„/ê¸°ê°„ ê´€ë ¨ ë‹¨ì–´ ì œì™¸
                if any(time_word in word for time_word in ["ë…„", "ë°˜ê¸°", "202"]):
                    continue
                # ê¸¸ì´ê°€ 2ê¸€ì ì´ìƒì´ê³  í•œê¸€, ì˜ë¬¸, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì í¬í•¨
                if len(word) >= 2 and re.match(r'^[\wê°€-í£Â·&]+$', word):
                    job = word
                    break
            
            logger.info(f"í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ - ê¸°ê°„: '{year_period}', ìƒíƒœ: '{status}', ì§ì—…: '{job}'")
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return year_period, status, job

    def extract_company_info_improved(self, soup):
        """NLP_Project ë°©ì‹ì„ ì ìš©í•œ ê°œì„ ëœ íšŒì‚¬ ì •ë³´ ì¶”ì¶œ"""
        company_name = ""
        period = ""
        position = ""
        job_role = ""
        
        try:
            # 1. íšŒì‚¬ëª… ì¶”ì¶œ - NLP_Project ë°©ì‹ ì ìš©
            company_name = self.extract_company_name_from_link(soup)
            
            # 2. ì§ë¬´ ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ - em íƒœê·¸ ìš°ì„ 
            position_text = self.extract_position_text_from_em(soup)
            
            # 3. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ë¶„ë¦¬
            if position_text:
                period, position, job_role = self.extract_position_info(position_text)
            
            # 4. ê¸°ë³¸ê°’ ì„¤ì •
            if not period:
                current_year = datetime.datetime.now().year
                period = f"{current_year}ë…„"
                
            logger.info(f"ê°œì„ ëœ ì¶”ì¶œ ê²°ê³¼ - íšŒì‚¬: '{company_name}', ê¸°ê°„: '{period}', ì§ìœ„: '{position}', ì§ë¬´: '{job_role}'")
            
        except Exception as e:
            logger.error(f"ê°œì„ ëœ íšŒì‚¬ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return company_name, period, position, job_role

    def clean_question_text(self, question_text):
        """ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°"""
        if not question_text:
            return ""
        
        # 'Q1.', 'Q2.' ë“± ì œê±°
        question_text = re.sub(r'^Q\d+\.?\s*', '', question_text)
        
        # 'ì§ˆë¬¸Q1.', 'ì§ˆë¬¸Q2.' ë“± ì œê±°  
        question_text = re.sub(r'^ì§ˆë¬¸Q\d+\.?\s*', '', question_text)
        
        # 'ë³´ê¸°' ë“± ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        question_text = re.sub(r'ë³´ê¸°$', '', question_text)
        
        return question_text.strip()

    def clean_answer_text(self, answer_text):
        """ë‹µë³€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì œê±°"""
        if not answer_text:
            return ""
        
        # ì•ì˜ 'ë‹µë³€' ì œê±°
        answer_text = re.sub(r'^ë‹µë³€\s*', '', answer_text)
        
        # ë’¤ì˜ ê¸€ììˆ˜ ì •ë³´ ì œê±° (ì˜ˆ: 'ê¸€ììˆ˜1,320ì2,066Byte')
        answer_text = re.sub(r'ê¸€ììˆ˜[\d,]+ì[\d,]+Byte$', '', answer_text)
        
        return answer_text.strip()

    def extract_clean_qa_pairs(self, soup):
        """NLP_Project ë°©ì‹ì˜ Q&A ì¶”ì¶œ ë° ì •ë¦¬"""
        questions = []
        answers = []

        try:
            # ìì†Œì„œ ë³¸ë¬¸ ì˜ì—­ë§Œ ì„ íƒ
            main_content = None
            content_selectors = [
                ".qnaLists",  # ë©”ì¸ Q&A ë¦¬ìŠ¤íŠ¸
                ".passAssayContent",  # ìì†Œì„œ ë‚´ìš©
                ".essay_content",  # ì—ì„¸ì´ ë‚´ìš©
                "#passAssayQnaLists",  # Q&A ë¦¬ìŠ¤íŠ¸ ID
                ".passAssayView .qna"  # ìì†Œì„œ ë·°ì˜ Q&A
            ]

            for selector in content_selectors:
                try:
                    main_content = soup.select_one(selector)
                    if main_content:
                        logger.info(f"ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ë°œê²¬: {selector}")
                        break
                except:
                    continue

            if not main_content:
                logger.info("ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ ì‹œë„")
                main_content = soup

            # dt(ì§ˆë¬¸), dd(ë‹µë³€) íƒœê·¸ ìŒìœ¼ë¡œ ì¶”ì¶œ
            question_elements = main_content.find_all('dt')
            answer_elements = main_content.find_all('dd')

            logger.info(f"ì§ˆë¬¸ ìš”ì†Œ ìˆ˜: {len(question_elements)}, ë‹µë³€ ìš”ì†Œ ìˆ˜: {len(answer_elements)}")

            for i in range(min(len(question_elements), len(answer_elements))):
                try:
                    # ì§ˆë¬¸ ì²˜ë¦¬
                    question_elem = question_elements[i]
                    question_text = self.extract_text_from_element(question_elem)

                    # ë‹µë³€ ì²˜ë¦¬
                    answer_elem = answer_elements[i]
                    answer_text = self.extract_text_from_element(answer_elem)

                    # ìœ íš¨í•œ ì§ˆë¬¸/ë‹µë³€ì¸ì§€ í™•ì¸ (ì—„ê²©í•œ í•„í„°ë§)
                    if self.is_valid_essay_qa(question_text, answer_text):
                        # í…ìŠ¤íŠ¸ ì •ë¦¬
                        cleaned_question = self.clean_question_text(question_text)
                        cleaned_answer = self.clean_answer_text(answer_text)
                        
                        questions.append(cleaned_question)
                        answers.append(cleaned_answer)
                        logger.info(f"ìœ íš¨í•œ Q{len(questions)}: {cleaned_question[:30]}...")
                    else:
                        logger.info(f"ë¬´íš¨í•œ ë°ì´í„° ì œì™¸: Q='{question_text}', A='{answer_text[:30]}'")

                except Exception as e:
                    logger.error(f"Q&A {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            return questions, answers

        except Exception as e:
            logger.error(f"Q&A ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return [], []

    def extract_text_from_element(self, element):
        """í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # .tx í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            tx_elem = element.select_one('.tx')
            if tx_elem:
                return tx_elem.get_text(strip=True)
        except:
            pass
        # ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
        return element.get_text(strip=True)

    def is_valid_essay_qa(self, question, answer):
        """ìœ íš¨í•œ ìì†Œì„œì¸ì§€ ê²€ì¦"""
        # 1ì°¨: ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì™„ì „ ì°¨ë‹¨
        invalid_keywords = [
            'ê³µì§€ì‚¬í•­', 'FAX', 'Email', 'email', 'ì „í™”ë²ˆí˜¸', 'ê³ ê°ì„¼í„°',
            'ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì¡ì½”ë¦¬ì•„', 'ì•Œë°”ëª¬', 'jobkorea',
            '02-565-9351', 'í‰ì¼ 09:00', 'í† ìš”ì¼ 09:00', '19:00', '15:00',
            'ì„¤ë¬¸ì´ë²¤íŠ¸', 'ë‹¹ì²¨ì', 'ë°œí‘œ', 'ì±„ìš©ë‹´ë‹¹ì', 'ì±„ìš© íŠ¸ë Œë“œ',
            'í‚¤ì›Œë“œ', 'ì‚¬ì¹­ ì—°ë½', 'ì£¼ì˜í•˜ì„¸ìš”', 'YYYYMMDD', '25.05',
            '(í‰ì¼', 'í† ìš”ì¼)', 'xì•Œë°”ëª¬', 'ë¬´ë£Œ'
        ]

        question_lower = question.lower()
        answer_lower = answer.lower()

        for keyword in invalid_keywords:
            if keyword.lower() in question_lower or keyword.lower() in answer_lower:
                return False

        # 2ì°¨: ìˆ«ìë‚˜ ë‚ ì§œë§Œ ìˆëŠ” ê²ƒ ì œì™¸
        if answer.replace(' ', '').replace('-', '').replace(':', '').isdigit():
            return False

        return True  # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ í—ˆìš©
    
    def extract_essay_content(self, essay_url):
        """ê°œë³„ ìì†Œì„œ ë‚´ìš© ì¶”ì¶œ - NLP_Project í†µí•© ë²„ì „"""
        logger.info(f"ìì†Œì„œ ì¶”ì¶œ ì‹œì‘: {essay_url}")
        
        try:
            response = self.session.get(essay_url)
            if response.status_code != 200:
                logger.error(f"í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            essay_data = {
                'url': essay_url,
                'company': '',
                'year_period': '',
                'status': '',
                'job': '',
                'questions': [],
                'answers': [],
                'essay_id': ''
            }
            
            # URLì—ì„œ essay_id ì¶”ì¶œ
            if "/View/" in essay_url:
                essay_data['essay_id'] = essay_url.split("/View/")[-1].split("?")[0]
            
            # íšŒì‚¬ëª… ì¶”ì¶œ - NLP_Project ë°©ì‹
            essay_data['company'] = self.extract_company_name_from_link(soup)
            
            # ì§ë¬´ ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ - NLP_Project ë°©ì‹
            position_text = self.extract_position_text_from_em(soup)
            
            # ì§ë¬´ ì •ë³´ íŒŒì‹± - NLP_Project ë°©ì‹
            try:
                year_period, status, job = self.extract_position_info(position_text)
                essay_data['year_period'] = year_period
                essay_data['status'] = status
                essay_data['job'] = job
                logger.info(f"ì¶”ì¶œëœ ì •ë³´ - ê¸°ê°„: '{year_period}', ìƒíƒœ: '{status}', ì§ì—…: '{job}'")
            except ValueError as e:
                logger.error(f"ì–¸íŒ¨í‚¹ ì˜¤ë¥˜: {e}")
                essay_data['year_period'] = ""
                essay_data['status'] = ""
                essay_data['job'] = ""
            
            # ì‹¤ì œ ìì†Œì„œ Q&Aë§Œ ì¶”ì¶œ - NLP_Project ë°©ì‹
            questions, answers = self.extract_clean_qa_pairs(soup)
            essay_data['questions'] = questions
            essay_data['answers'] = answers
            
            return essay_data if questions else None
            
        except Exception as e:
            logger.error(f"ìì†Œì„œ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {
                'url': essay_url,
                'company': '',
                'year_period': '',
                'status': '',
                'job': '',
                'questions': [],
                'answers': [],
                'essay_id': ''
            }
    
    def get_essay_links_from_page(self, page_url):
        """í˜ì´ì§€ì—ì„œ ìì†Œì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            response = self.session.get(page_url)
            if response.status_code != 200:
                logger.error(f"í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            essay_links = []
            
            # ìì†Œì„œ ë§í¬ ì°¾ê¸°
            selectors = [
                'a[href*="/starter/PassAssay/View/"]',
                'a[href*="PassAssay/View"]',
                'a[href*="passassay/View"]',
                'a[href*="passassay/view"]'
            ]
            
            for selector in selectors:
                try:
                    links = soup.select(selector)
                    for link in links:
                        href = link.get('href')
                        if href:
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            full_url = urljoin(page_url, href)
                            if any(keyword in full_url for keyword in ['PassAssay/View', 'passassay/View', 'passassay/view']):
                                essay_links.append(full_url)
                    
                    if essay_links:
                        break
                except:
                    continue
            
            # ì¤‘ë³µ ì œê±°
            essay_links = list(set(essay_links))
            logger.info(f"í˜ì´ì§€ì—ì„œ {len(essay_links)}ê°œ ìì†Œì„œ ë§í¬ ë°œê²¬")
            
            return essay_links
            
        except Exception as e:
            logger.error(f"ë§í¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def scrape_jobkorea_with_login(self, user_id, password, start_page=1, end_page=10):
        """ë©”ì¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜"""
        all_essays = []
        
        try:
            # ë¡œê·¸ì¸
            logger.info("=== ë¡œê·¸ì¸ ì‹œë„ ===")
            if not self.login_to_jobkorea(user_id, password):
                logger.error("ë¡œê·¸ì¸ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return []
            
            logger.info("\n=== í•©ê²©ìì†Œì„œ í˜ì´ì§€ë¡œ ì´ë™ ===")
            base_url = "https://www.jobkorea.co.kr/starter/passassay?schTxt=&Page="
            
            for page in range(start_page, end_page + 1):
                logger.info(f"\ní˜ì´ì§€ {page}/{end_page} ì²˜ë¦¬ ì¤‘...")
                
                page_url = base_url + str(page)
                
                # ìì†Œì„œ ë§í¬ ì°¾ê¸°
                essay_links = self.get_essay_links_from_page(page_url)
                
                if not essay_links:
                    logger.warning("ìì†Œì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ê° ìì†Œì„œ ì²˜ë¦¬ (í˜ì´ì§€ë‹¹ ìµœëŒ€ 3ê°œ)
                for i, essay_url in enumerate(essay_links[:3]):
                    logger.info(f"\n  {i+1}/{min(len(essay_links), 3)} ìì†Œì„œ ì²˜ë¦¬...")
                    logger.info(f"  URL: {essay_url}")
                    
                    essay_data = self.extract_essay_content(essay_url)
                    
                    if essay_data and essay_data['questions']:
                        all_essays.append(essay_data)
                        logger.info(f"    âœ… ì„±ê³µ!")
                        logger.info(f"       íšŒì‚¬: '{essay_data['company']}'")
                        logger.info(f"       ì—°ë„: '{essay_data['year_period']}'")
                        logger.info(f"       ìƒíƒœ: '{essay_data['status']}'")
                        logger.info(f"       ì§ì—…: '{essay_data['job']}'")
                        logger.info(f"       ë¬¸í•­ìˆ˜: {len(essay_data['questions'])}")
                    else:
                        logger.warning(f"    âŒ ì‹¤íŒ¨ ë˜ëŠ” ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ")
                    
                    time.sleep(2)  # ìš”ì²­ ê°„ê²©
        
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        
        return all_essays

def save_to_excel(essays, save_directory='./data/', start_page=1, end_page=10):
    """ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(save_directory, exist_ok=True)
    
    if not essays:
        logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ë³€í™˜ - ì‚¬ìš©ìê°€ ìš”ì²­í•œ êµ¬ì¡°ë¡œ
    df_data = []
    successful_essays = 0
    
    for essay in essays:
        if essay and essay['questions'] and essay['answers']:
            successful_essays += 1
            for i, (question, answer) in enumerate(zip(essay['questions'], essay['answers'])):
                df_data.append({
                    'íšŒì‚¬ëª…': essay['company'],
                    'ê¸°ê°„': essay['year_period'], 
                    'ì§ìœ„': essay['status'],
                    'ì§ë¬´': essay['job'],
                    'ì§ˆë¬¸ë²ˆí˜¸': f'Q{i + 1}',
                    'ì§ˆë¬¸': question,
                    'ë‹µë³€': answer,
                    'URL': essay['url'],
                    'Essay_ID': essay['essay_id']
                })
    
    if not df_data:
        logger.warning("ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(df_data)
    
    # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
    logger.info(f"\n=== ë°ì´í„° í’ˆì§ˆ ìƒì„¸ ë¶„ì„ ===")
    logger.info(f"ì´ í–‰ ìˆ˜: {len(df)}")
    logger.info(f"ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ìì†Œì„œ: {successful_essays}/{len(essays)}")
    
    # íšŒì‚¬ëª… ë¶„ì„
    companies_filled = df[df['íšŒì‚¬ëª…'].str.len() > 0]
    logger.info(f"íšŒì‚¬ëª… ì±„ì›Œì§„ í–‰: {len(companies_filled)}/{len(df)} ({len(companies_filled)/len(df)*100:.1f}%)")
    
    if len(companies_filled) > 0:
        unique_companies = df['íšŒì‚¬ëª…'].value_counts()
        logger.info(f"ìœ ë‹ˆí¬ íšŒì‚¬ ìˆ˜: {len(unique_companies)}")
        logger.info("ìƒìœ„ íšŒì‚¬ë“¤:")
        for company, count in unique_companies.head(5).items():
            if company:
                logger.info(f"  - {company}: {count}ê°œ ë¬¸í•­")
    
    # ì§ˆë¬¸/ë‹µë³€ ê¸¸ì´ ë¶„ì„
    avg_question_len = df['ì§ˆë¬¸'].str.len().mean()
    avg_answer_len = df['ë‹µë³€'].str.len().mean()
    logger.info(f"í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_question_len:.0f}ì")
    logger.info(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_answer_len:.0f}ì")
    
    # Excel íŒŒì¼ ì €ì¥
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ëœ íŒŒì¼
    excel_filename_detailed = f'ì¡ì½”ë¦¬ì•„_í•©ê²©ìì†Œì„œ_p{start_page}-{end_page}_{timestamp}.xlsx'
    excel_path_detailed = os.path.join(save_directory, excel_filename_detailed)
    
    # ê³ ì •ëœ íŒŒì¼ëª… (ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©)
    excel_filename_simple = 'ì¡ì½”ë¦¬ì•„_í•©ê²©ìì†Œì„œ.xlsx'
    excel_path_simple = os.path.join(save_directory, excel_filename_simple)
    
    if os.path.exists(excel_path_simple):
        # ê¸°ì¡´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
        existing_df = pd.read_excel(excel_path_simple)
        logger.info(f"\nê¸°ì¡´ ë°ì´í„° {len(existing_df)}í–‰ + ì‹ ê·œ ë°ì´í„° {len(df)}í–‰ â†’ ë³‘í•© ì¤‘...")
        combined_df = pd.concat([existing_df, df], ignore_index=True)
    else:
        combined_df = df
        logger.info("\nìµœì´ˆ Excel íŒŒì¼ ìƒì„±")
    
    # Essay_ID ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    before_dedup = len(combined_df)
    combined_df = combined_df.drop_duplicates(subset=['Essay_ID'], keep='first')
    after_dedup = len(combined_df)
    removed_count = before_dedup - after_dedup
    
    if removed_count > 0:
        logger.info(f"ğŸ” ì¤‘ë³µ ë°ì´í„° {removed_count}í–‰ ì œê±° â†’ ìµœì¢… {after_dedup}í–‰")
    else:
        logger.info(f"ì¤‘ë³µ ë°ì´í„° ì—†ìŒ â†’ ìµœì¢… {after_dedup}í–‰")
    
    # Excel íŒŒì¼ë¡œ ì €ì¥ (ì—´ ë„ˆë¹„ ìë™ ì¡°ì •)
    with pd.ExcelWriter(excel_path_detailed, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='í•©ê²©ìì†Œì„œ', index=False)
        
        # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
        worksheet = writer.sheets['í•©ê²©ìì†Œì„œ']
        
        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
        for column in worksheet.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            
            # ìµœëŒ€ ë„ˆë¹„ 50ìœ¼ë¡œ ì œí•œ
            adjusted_width = min(max_length + 2, 50)
            worksheet.column_dimensions[column_letter].width = adjusted_width
    
    # ëˆ„ì  ë°ì´í„° íŒŒì¼ ì €ì¥
    with pd.ExcelWriter(excel_path_simple, engine='openpyxl') as writer:
        combined_df.to_excel(writer, sheet_name='í•©ê²©ìì†Œì„œ', index=False)
        
        # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
        worksheet = writer.sheets['í•©ê²©ìì†Œì„œ']
        
        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
        for column in worksheet.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            
            # ìµœëŒ€ ë„ˆë¹„ 50ìœ¼ë¡œ ì œí•œ
            adjusted_width = min(max_length + 2, 50)
            worksheet.column_dimensions[column_letter].width = adjusted_width
    
    logger.info(f"\nğŸ’¾ Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ:")
    logger.info(f"   ğŸ“„ ìƒì„¸ íŒŒì¼: {excel_filename_detailed} (ì‹ ê·œ ë°ì´í„°ë§Œ)")
    logger.info(f"   ğŸ“„ ê¸°ë³¸ íŒŒì¼: {excel_filename_simple} (ëˆ„ì  ë°ì´í„° {after_dedup}í–‰)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì •
    save_directory = './data/'
    os.makedirs(save_directory, exist_ok=True)
    
    # ë¡œê·¸ì¸ ì •ë³´ ì…ë ¥ë°›ê¸°
    print("=== ì¡ì½”ë¦¬ì•„ í•©ê²©ìì†Œì„œ ìŠ¤í¬ë˜í•‘ í”„ë¡œê·¸ë¨ ===")
    print("NLP_Project ë°©ì‹ ì™„ì „ í†µí•© ë²„ì „ (Python 3.13.3 í˜¸í™˜)")
    print("ë¡œê·¸ì¸ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
    
    user_id = input("ì¡ì½”ë¦¬ì•„ ì•„ì´ë””: ").strip()
    if not user_id:
        print("âŒ ì•„ì´ë””ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    password = getpass.getpass("ì¡ì½”ë¦¬ì•„ ë¹„ë°€ë²ˆí˜¸: ").strip()
    if not password:
        print("âŒ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    # ìŠ¤í¬ë˜í•‘ í˜ì´ì§€ ë²”ìœ„ ì„¤ì •
    print("\nìŠ¤í¬ë˜í•‘ ë²”ìœ„ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    try:
        start_page = int(input("ì‹œì‘ í˜ì´ì§€ (ê¸°ë³¸ê°’: 1): ") or "1")
        end_page = int(input("ë í˜ì´ì§€ (ê¸°ë³¸ê°’: 5): ") or "5")
        
        if start_page < 1 or end_page < start_page:
            print("âŒ ì˜¬ë°”ë¥¸ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
            
    except ValueError:
        print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    logger.info(f"\n=== ì¡ì½”ë¦¬ì•„ í•©ê²©ìì†Œì„œ ìŠ¤í¬ë˜í•‘ ({start_page}~{end_page}í˜ì´ì§€) ===")
    start_time = datetime.datetime.now()
    
    try:
        scraper = JobkoreaScraper()
        essays = scraper.scrape_jobkorea_with_login(user_id, password, start_page=start_page, end_page=end_page)
        
        if essays:
            logger.info(f"\nâœ… ì´ {len(essays)}ê°œì˜ ìì†Œì„œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            save_to_excel(essays, save_directory, start_page, end_page)
            
            # ì™„ë£Œ ì‹œê°„ ê³„ì‚°
            end_time = datetime.datetime.now()
            duration = end_time - start_time
            
            logger.info(f"\nğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ í˜ì´ì§€ ë²”ìœ„: {start_page}~{end_page}")
            logger.info(f"ğŸ“ ì´ ìì†Œì„œ ìˆ˜: {len(essays)}")
            
        else:
            logger.warning("ìˆ˜ì§‘ëœ ìì†Œì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
